{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b52e10",
   "metadata": {},
   "source": [
    "# ARCHI_PRODUCT SCRAPING SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f3ea8",
   "metadata": {},
   "source": [
    " # WITH IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────────\n",
    "OUTPUT_CSV = \"poo.csv\"\n",
    "HEADERS = [\n",
    "    \"Title\", \"ProductName\", \"producturl\", \"Brand\", \"Type\",\n",
    "    \"Category\", \"Description\", \"Tags\", \"ImageURLs\",\n",
    "    \"DimensionText\", \"DimensionImages\"\n",
    "]\n",
    "BRANDS = [\n",
    "    \"Ditre-Italia\",\"Cattelan-Italia\",\"Vondom\",\"NICOLINE\",\"Natuzzi-Italia\",\"Talenti\",\n",
    "    \"Cappellini\",\"Opinion-Ciatti\",\"DIESEL\",\"Bontempi-Casa\",\"Calligaris\",\"Pianca\",\n",
    "    \"Moroso\",\"GUFRAM\",\"FontanaArte\",\"LODES\",\"Vibia\",\"LAGO\",\"Seletti\",\"Bonaldo\",\n",
    "    \"Arketipo\",\"SLIDE\",\"Tonelli-Design\",\"Kose\",\"Foscarini\",\"Zeus\",\"Tomasella\",\n",
    "    \"Lapalma\",\"Sovet-italia\",\"CAMERICH\",\"Connubia\",\"Mogg\",\"Driade\",\"Marset\",\"Oluce\",\n",
    "    \"Flos\",\"Caracole\",\"Reflex\",\"Desalto\",\"Lema\",\"Flou\",\"UNOPIU\",\"Alias\",\n",
    "    (\"Wiener GTV Designa\",\"https://www.archiproducts.com/en/products?q=wiener%20gtv%20design\"),\n",
    "    (\"MIDS\",\"https://www.archiproducts.com/en/products?q=MIDS\"),\n",
    "    \"Fiam-Italia\",\"Il-Fanale\",\"Qeeboo\",\"Saba Italia\",\"Magis\",\"IMPACT-ACOUSTIC\",\n",
    "]\n",
    "\n",
    "# ─── HELPERS ───────────────────────────────────────────────────────────────────\n",
    "def init_driver():\n",
    "    opts = uc.ChromeOptions()\n",
    "    # opts.add_argument(\"--headless\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return uc.Chrome(options=opts)\n",
    "\n",
    "def ensure_csv():\n",
    "    if not os.path.isfile(OUTPUT_CSV):\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.DictWriter(f, fieldnames=HEADERS).writeheader()\n",
    "\n",
    "def append_row(row):\n",
    "    with open(OUTPUT_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.DictWriter(f, fieldnames=HEADERS).writerow(row)\n",
    "\n",
    "def get_text(driver, xpath, timeout=5):\n",
    "    try:\n",
    "        el = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.XPATH, xpath))\n",
    "        )\n",
    "        return el.text.strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_texts(driver, xpath):\n",
    "    try:\n",
    "        return [el.text.strip() for el in driver.find_elements(By.XPATH, xpath) if el.text.strip()]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# ─── MAIN SCRAPER ───────────────────────────────────────────────────────────────\n",
    "def scrape():\n",
    "    driver = init_driver()\n",
    "    ensure_csv()\n",
    "\n",
    "    for b in BRANDS:\n",
    "        if isinstance(b, tuple):\n",
    "            brand_key, listing_url = b\n",
    "        else:\n",
    "            brand_key = b\n",
    "            listing_url = f\"https://www.archiproducts.com/en/{b}/products\"\n",
    "\n",
    "        print(f\"\\n→ Scraping brand: {brand_key}\")\n",
    "        driver.get(listing_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, \"#productGrid > div.cell._search-item a\")\n",
    "            links = {a.get_attribute(\"href\") for a in cards if a.get_attribute(\"href\")}\n",
    "            print(f\" • Found {len(links)} products\")\n",
    "\n",
    "            for url in links:\n",
    "                print(\"   -\", url)\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "\n",
    "                row = {\n",
    "                    \"Title\": get_text(driver, \"//article//h2\"),\n",
    "                    \"ProductName\": get_text(driver, \"//hgroup/h1/span[2]\"),\n",
    "                    \"producturl\": url,\n",
    "                    \"Brand\": get_text(driver, \"//hgroup/h1/span[1]/a\"),\n",
    "                    \"Type\": get_text(driver, \"//hgroup/h1/span[3]\"),\n",
    "                    \"Category\": get_text(driver, \"//nav//li[4]/a/span\"),\n",
    "                    \"Description\": get_text(driver, \"//section/div[1]/article/div[2]//div\"),\n",
    "                }\n",
    "\n",
    "                # TAGS (dynamic by header, fallback)\n",
    "                tags = []\n",
    "                try:\n",
    "                    hdr = driver.find_element(By.XPATH, \"//a[normalize-space(text())='Tags']\")\n",
    "                    panel = hdr.find_element(By.XPATH, \"./ancestor::div[contains(@class,'accordion-item')]\")\n",
    "                    for a in panel.find_elements(By.XPATH, \".//div[contains(@class,'accordion-content')]//a\"):\n",
    "                        txt = a.text.strip()\n",
    "                        if txt:\n",
    "                            tags.append(txt)\n",
    "                except:\n",
    "                    tags = get_texts(driver, \"//section/div[1]/section/div[6]//a/span\")\n",
    "                row[\"Tags\"] = \";\".join(tags)\n",
    "\n",
    "                # IMAGES — only .jpeg URLs\n",
    "                imgs = []\n",
    "                img_els = driver.find_elements(By.CSS_SELECTOR,\n",
    "                    \"div.productsheet__overview__gallery div.image-container img\"\n",
    "                )\n",
    "                for img in img_els:\n",
    "                    src = img.get_attribute(\"src\") or img.get_attribute(\"data-src\")\n",
    "                    if src and src.startswith(\"http\") and src.lower().endswith(\".jpeg\"):\n",
    "                        if src not in imgs:\n",
    "                            imgs.append(src)\n",
    "                row[\"ImageURLs\"] = \";\".join(imgs)\n",
    "\n",
    "                # DIMENSIONS (using provided XPath for text)\n",
    "                if driver.find_elements(By.XPATH, \"//a[normalize-space(text())='Dimensions']\"):\n",
    "                    row[\"DimensionText\"] = get_text(driver,\n",
    "                        \"/html/body/section[2]/div[1]/div/div/div[2]/div[1]/section/div[2]/div\"\n",
    "                    ).replace(\"\\n\", \" | \").strip()\n",
    "\n",
    "                    dim_imgs = []\n",
    "                    for e in driver.find_elements(By.XPATH,\n",
    "                        \"//a[normalize-space(text())='Dimensions']/ancestor::div[contains(@class,'accordion-item')]\"\n",
    "                        \"//figure//a/img\"\n",
    "                    ):\n",
    "                        src = e.get_attribute(\"src\")\n",
    "                        if src and src.startswith(\"http\") and src.lower().endswith(\".jpeg\"):\n",
    "                            dim_imgs.append(src)\n",
    "                    row[\"DimensionImages\"] = \";\".join(dim_imgs)\n",
    "                else:\n",
    "                    row[\"DimensionText\"] = \"\"\n",
    "                    row[\"DimensionImages\"] = \"\"\n",
    "\n",
    "                append_row(row)\n",
    "                driver.back()\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Next page\n",
    "            try:\n",
    "                nxt = driver.find_element(By.LINK_TEXT, \"Next\")\n",
    "                if \"disabled\" in nxt.get_attribute(\"class\"):\n",
    "                    break\n",
    "                nxt.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        print(f\"✔ Completed brand: {brand_key}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87557a54",
   "metadata": {},
   "source": [
    "#  Images  script  for  high resolutions images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "INPUT_CSV  = \"do_final_merged_complete_mapped.csv\"\n",
    "OUTPUT_CSV = \"do_rescraping_image_url.csv\"\n",
    "RETRY_DELAY = 5  # seconds\n",
    "\n",
    "# ─── SELENIUM HELPERS ───────────────────────────────────────────────────────────\n",
    "def init_driver():\n",
    "    opts = uc.ChromeOptions()\n",
    "    # opts.add_argument(\"--headless\")  # Optional\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = uc.Chrome(options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "def wait_for_xpath(driver, xpath, timeout=20):\n",
    "    WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((By.XPATH, xpath))\n",
    "    )\n",
    "\n",
    "def load_with_retry(driver, url, xpaths):\n",
    "    if isinstance(xpaths, str):\n",
    "        xpaths = [xpaths]\n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            for xp in xpaths:\n",
    "                try:\n",
    "                    wait_for_xpath(driver, xp)\n",
    "                    return\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            raise TimeoutException(f\"None of {xpaths} found\")\n",
    "        except TimeoutException as e:\n",
    "            print(f\"  ⚠ Timeout loading {url} ({e}), retrying in {RETRY_DELAY}s...\")\n",
    "            try:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(RETRY_DELAY)\n",
    "\n",
    "# ─── URL HELPERS (ensure high-res) ─────────────────────────────────────────────\n",
    "ALLOWED_EXTS = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "\n",
    "def strip_query(u: str) -> str:\n",
    "    return (u or \"\").split(\"?\")[0].strip()\n",
    "\n",
    "def promote_variant(u: str) -> str:\n",
    "    # Archiproducts thumbnails often use `/g/`. Prefer `/p/`.\n",
    "    if \"/g/\" in u:\n",
    "        return u.replace(\"/g/\", \"/p/\")\n",
    "    return u\n",
    "\n",
    "def parse_srcset(srcset: str) -> str:\n",
    "    try:\n",
    "        parts = [p.strip() for p in (srcset or \"\").split(\",\") if p.strip()]\n",
    "        if not parts:\n",
    "            return \"\"\n",
    "        # take the last (largest) candidate\n",
    "        return strip_query(parts[-1].split()[0])\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def is_valid_image(u: str) -> bool:\n",
    "    return u.lower().startswith(\"http\") and u.lower().endswith(ALLOWED_EXTS)\n",
    "\n",
    "def best_src(el) -> str:\n",
    "    # Prefer the largest/best attributes first\n",
    "    candidates = [\n",
    "        el.get_attribute(\"data-src-big\"),\n",
    "        el.get_attribute(\"data-zoom\"),\n",
    "        el.get_attribute(\"data-large\"),\n",
    "        el.get_attribute(\"data-full\"),\n",
    "        parse_srcset(el.get_attribute(\"srcset\")),\n",
    "        el.get_attribute(\"data-src\"),\n",
    "        el.get_attribute(\"src\"),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        u = promote_variant(strip_query(c or \"\"))\n",
    "        if is_valid_image(u):\n",
    "            return u\n",
    "\n",
    "    # last resort: nearest anchor href\n",
    "    try:\n",
    "        anchor = el.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "        href = promote_variant(strip_query(anchor.get_attribute(\"href\") or \"\"))\n",
    "        if is_valid_image(href):\n",
    "            return href\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def dedupe_preserve(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def drop_thumbs_if_hires(urls):\n",
    "    # If any non-/g/ URLs exist, drop /g/ thumbnails\n",
    "    hires_exists = any(\"/g/\" not in u for u in urls)\n",
    "    if hires_exists:\n",
    "        return [u for u in urls if \"/g/\" not in u]\n",
    "    return urls\n",
    "\n",
    "# ─── MAIN IMAGE SCRAPER ─────────────────────────────────────────────────────────\n",
    "def scrape_images(driver):\n",
    "    # Scroll to bottom for lazy-loaded images\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    collected = []\n",
    "\n",
    "    # 1) MAIN GALLERY (#product-image)\n",
    "    try:\n",
    "        gallery = driver.find_element(By.ID, \"product-image\")\n",
    "        imgs = gallery.find_elements(By.TAG_NAME, \"img\")\n",
    "    except:\n",
    "        imgs = []\n",
    "    for img in imgs:\n",
    "        u = best_src(img)\n",
    "        if u:\n",
    "            collected.append(u)\n",
    "\n",
    "    # 2) FLICKITY CAROUSEL (fallback)\n",
    "    if not collected:\n",
    "        flicks = driver.find_elements(By.CSS_SELECTOR, \"div.flickity-slider .carousel-cell img\")\n",
    "        for img in flicks:\n",
    "            u = best_src(img)\n",
    "            if u:\n",
    "                collected.append(u)\n",
    "\n",
    "    # 3) OVERVIEW GALLERY (fallback)\n",
    "    if not collected:\n",
    "        ov_containers = []\n",
    "        ov_containers += driver.find_elements(By.CSS_SELECTOR, \"div.productsheet_overview_gallery\")\n",
    "        ov_containers += driver.find_elements(By.CSS_SELECTOR, \"div.productsheet__overview__gallery\")\n",
    "        imgs2 = []\n",
    "        for ov in ov_containers:\n",
    "            try:\n",
    "                imgs2 += ov.find_elements(By.TAG_NAME, \"img\")\n",
    "            except:\n",
    "                pass\n",
    "        for img in imgs2:\n",
    "            u = best_src(img)\n",
    "            if u:\n",
    "                collected.append(u)\n",
    "\n",
    "    # 4) Single direct image fallback\n",
    "    if not collected:\n",
    "        try:\n",
    "            img = driver.find_element(By.ID, \"imgCarousel\")\n",
    "            u = best_src(img)\n",
    "            if u:\n",
    "                collected.append(u)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Clean up + drop thumbnails if any hi-res exists\n",
    "    collected = dedupe_preserve(collected)\n",
    "    collected = drop_thumbs_if_hires(collected)\n",
    "    return collected\n",
    "\n",
    "# ─── DIMENSION IMAGE SCRAPER ───────────────────────────────────────────────────\n",
    "def scrape_dimensions(driver):\n",
    "    raw = []\n",
    "    if driver.find_elements(By.XPATH, \"//a[normalize-space(text())='Dimensions']\"):\n",
    "        els = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//div[contains(@class,'accordion-item') and .//a[text()='Dimensions']]//figure//a/img\"\n",
    "        )\n",
    "        for img in els:\n",
    "            u = best_src(img)\n",
    "            if u:\n",
    "                raw.append(u)\n",
    "    raw = dedupe_preserve(raw)\n",
    "    raw = drop_thumbs_if_hires(raw)\n",
    "    return raw\n",
    "\n",
    "# ─── MAIN FUNCTION ─────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # Read input rows\n",
    "    with open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "        fieldnames = reader.fieldnames.copy()\n",
    "\n",
    "    # Ensure columns exist\n",
    "    if \"ImageURLs\" not in fieldnames:\n",
    "        fieldnames.append(\"ImageURLs\")\n",
    "    if \"DimensionImages\" not in fieldnames:\n",
    "        fieldnames.append(\"DimensionImages\")\n",
    "\n",
    "    driver = init_driver()\n",
    "\n",
    "    # Overwrite output and start fresh\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        url_cache = {}  # producturl -> (img_str, dim_str)\n",
    "\n",
    "        total = len(rows)\n",
    "        try:\n",
    "            for idx, row in enumerate(rows, start=1):\n",
    "                url = (row.get(\"producturl\") or \"\").strip()\n",
    "\n",
    "                if not url:\n",
    "                    print(f\"[{idx}/{total}] ⚠ Empty producturl, skipping row (not written).\")\n",
    "                    continue\n",
    "\n",
    "                # Skip scraping duplicates; reuse first result\n",
    "                if url in url_cache:\n",
    "                    img_str, dim_str = url_cache[url]\n",
    "                    print(f\"[{idx}/{total}] ⏭ Duplicate URL, reusing cached result: {url}\")\n",
    "                    row[\"ImageURLs\"] = img_str\n",
    "                    row[\"DimensionImages\"] = dim_str\n",
    "                    writer.writerow(row)\n",
    "                    f.flush(); os.fsync(f.fileno())\n",
    "                    continue\n",
    "\n",
    "                print(f\"[{idx}/{total}] Processing: {url}\")\n",
    "\n",
    "                try:\n",
    "                    load_with_retry(driver, url, [\"//*[@id='product-image']\", \"//hgroup/h1/span[2]\"])\n",
    "                    imgs = scrape_images(driver)\n",
    "                    dims = scrape_dimensions(driver)\n",
    "\n",
    "                    # Live update line (restored)\n",
    "                    print(f\"    → Found {len(imgs)} main images, {len(dims)} dimension images\")\n",
    "\n",
    "                    if not imgs:\n",
    "                        print(f\"    → 0 images found. Skipping write for this row.\")\n",
    "                        continue\n",
    "\n",
    "                    img_str = \";\".join(imgs)\n",
    "                    dim_str = \";\".join(dims)\n",
    "                    row[\"ImageURLs\"] = img_str\n",
    "                    row[\"DimensionImages\"] = dim_str\n",
    "\n",
    "                    # cache for duplicates\n",
    "                    url_cache[url] = (img_str, dim_str)\n",
    "\n",
    "                    writer.writerow(row)\n",
    "                    f.flush(); os.fsync(f.fileno())\n",
    "                    time.sleep(random.uniform(1, 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    ❌ Error on {url}: {e}\")\n",
    "                    # do not write empty/failed rows; continue to next\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n🛑 Stopped by user. Exiting cleanly without writing partial/empty rows...\")\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\n✅ Done! Output saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97632ac2",
   "metadata": {},
   "source": [
    "# Downloding images script of products and placing in folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e866175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "import cloudscraper\n",
    "import re\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────\n",
    "CSV_FILE = \"final_merged_complete.csv\"\n",
    "IMAGE_ROOT = \"images\"\n",
    "\n",
    "# ─── Initialize Cloudflare Scraper ────────────────────────\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "# ─── Sanitize folder/file names ───────────────────────────\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '', name.strip()).replace(\" \", \"\")\n",
    "\n",
    "# ─── Download image bypassing Cloudflare ──────────────────\n",
    "def download_image(url, save_path):\n",
    "    try:\n",
    "        if not url.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            print(f\"⏩ Skipped non-image URL: {url}\")\n",
    "            return\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"⏩ Already exists: {save_path}\")\n",
    "            return\n",
    "\n",
    "        response = scraper.get(url, stream=True, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                shutil.copyfileobj(response.raw, f)\n",
    "            print(f\"✅ Saved: {save_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed: {url} → {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error downloading {url}: {e}\")\n",
    "\n",
    "# ─── Check if all expected images exist ───────────────────\n",
    "def all_images_downloaded(folder, num_main, num_dim):\n",
    "    main_found = sum(1 for f in os.listdir(folder) if f.startswith(\"main_\") and f.lower().endswith((\".jpg\", \".jpeg\", \".png\")))\n",
    "    dim_found = sum(1 for f in os.listdir(folder) if f.startswith(\"dimension_\") and f.lower().endswith((\".jpg\", \".jpeg\", \".png\")))\n",
    "    return main_found >= num_main and dim_found >= num_dim\n",
    "\n",
    "# ─── Main Execution ───────────────────────────────────────\n",
    "with open(CSV_FILE, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "\n",
    "    for row in reader:\n",
    "        brand = sanitize_filename(row[\"Brand\"])\n",
    "        product = sanitize_filename(row[\"ProductName\"])\n",
    "        category = sanitize_filename(row[\"Category\"])\n",
    "        type_ = sanitize_filename(row[\"Type\"])\n",
    "\n",
    "        # Final folder structure: Brand/Product/Category/Type\n",
    "        folder_path = os.path.join(IMAGE_ROOT, brand, product, category, type_)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        image_urls = row[\"ImageURLs\"].split(\";\") if row.get(\"ImageURLs\") else []\n",
    "        dim_urls = row[\"DimensionImages\"].split(\";\") if row.get(\"DimensionImages\") else []\n",
    "\n",
    "        if all_images_downloaded(folder_path, len(image_urls), len(dim_urls)):\n",
    "            print(f\"🔁 Skipped (all images already downloaded): {brand}/{product}/{category}/{type_}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n📂 Processing: {brand}/{product}/{category}/{type_}\")\n",
    "\n",
    "        # ─── Download main images ─────\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            if url.strip():\n",
    "                filename = f\"main_{idx+1}.jpg\"\n",
    "                save_path = os.path.join(folder_path, filename)\n",
    "                download_image(url.strip(), save_path)\n",
    "\n",
    "        # ─── Download dimension images ─────\n",
    "        for idx, url in enumerate(dim_urls):\n",
    "            if url.strip():\n",
    "                filename = f\"dimension_{idx+1}.jpg\"\n",
    "                save_path = os.path.join(folder_path, filename)\n",
    "                download_image(url.stri p(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0dc07",
   "metadata": {},
   "source": [
    "# Images mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1798d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────\n",
    "INPUT_CSV = \"Archi_product_scraping_moiz.csv\"\n",
    "OUTPUT_CSV = \"MAPPED_ARCHI.csv\"\n",
    "DOMAIN = \"https://italcasa.us\"\n",
    "IMAGE_FOLDER_ROOT = \"images\"\n",
    "MAIN_IMAGE_COUNT = 5           # main_1.jpg ... main_5.jpg\n",
    "MAX_DIM_IMAGES = 5             # limit mapped dimension images\n",
    "\n",
    "# ─── SANITIZE FUNCTION ─────────────────────────────────\n",
    "def sanitize(text):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"Unknown\"\n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'[<>:\\\"/\\\\|?*]', \"_\", text)\n",
    "    return text.replace(\" \", \"_\")\n",
    "\n",
    "# ─── CLEAN (no garbage removal) ────────────────────────\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    return str(text).strip()\n",
    "\n",
    "# ─── HELPERS ───────────────────────────────────────────\n",
    "def split_urls(maybe_urls: str):\n",
    "    \"\"\"Split a string of URLs on ',', ';', '|', or whitespace.\"\"\"\n",
    "    if not maybe_urls or not isinstance(maybe_urls, str):\n",
    "        return []\n",
    "    raw = re.split(r\"[,\\;\\|\\s]+\", maybe_urls.strip())\n",
    "    return [u for u in (s.strip() for s in raw) if u]\n",
    "\n",
    "# ─── MAIN ──────────────────────────────────────────────\n",
    "def generate_updated_csv():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    # Expected input columns\n",
    "    required = [\n",
    "        \"Title\", \"ProductName\", \"producturl\", \"Brand\", \"Type\", \"Category\",\n",
    "        \"Description\", \"tax:product_tag\", \"ImageURLs\",\n",
    "        \"attribute:Dimensions:\", \"attribute_data:Dimensions:\", \"DimensionImages\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        brand = sanitize(row.get(\"Brand\", \"\"))\n",
    "        product = sanitize(row.get(\"ProductName\", \"\"))\n",
    "        category = sanitize(row.get(\"Category\", \"\"))\n",
    "        ptype_raw = row.get(\"Type\", \"\")\n",
    "        ptype = sanitize(ptype_raw) if str(ptype_raw).strip() else \"Unknown\"\n",
    "\n",
    "        base_url = f\"{DOMAIN}/wp-content/uploads/{IMAGE_FOLDER_ROOT}/{brand}/{product}/{category}/{ptype}\"\n",
    "\n",
    "        # Main images: main_1.jpg ... main_N.jpg\n",
    "        main_urls = [f\"{base_url}/main_{i}.jpg\" for i in range(1, MAIN_IMAGE_COUNT + 1)]\n",
    "\n",
    "        # Dimension images: always dimension_1.jpg, dimension_2.jpg ... (force .jpg)\n",
    "        src_dim = clean_text(row.get(\"DimensionImages\", \"\"))\n",
    "        src_dim_urls = split_urls(src_dim)\n",
    "        mapped_dim_urls = []\n",
    "        for i, _ in enumerate(src_dim_urls, start=1):\n",
    "            if i > MAX_DIM_IMAGES:\n",
    "                break\n",
    "            mapped_dim_urls.append(f\"{base_url}/dimension_{i}.jpg\")\n",
    "\n",
    "        # Copy/clean other fields\n",
    "        title = clean_text(row.get(\"Title\", \"\"))\n",
    "        productname = clean_text(row.get(\"ProductName\", \"\"))\n",
    "        producturl = clean_text(row.get(\"producturl\", \"\"))\n",
    "        brand_txt = clean_text(row.get(\"Brand\", \"\"))\n",
    "        type_txt = clean_text(row.get(\"Type\", \"\"))\n",
    "        category_txt = clean_text(row.get(\"Category\", \"\"))\n",
    "        desc = clean_text(row.get(\"Description\", \"\"))\n",
    "\n",
    "        # Tags column is 'tax:product_tag'\n",
    "        tags_raw = clean_text(row.get(\"tax:product_tag\", \"\"))\n",
    "        tags_norm = tags_raw.replace(\";\", \"|\") if tags_raw else \"\"\n",
    "\n",
    "        # Dimension text & data copied as-is\n",
    "        dim_text = clean_text(row.get(\"attribute:Dimensions:\", \"\"))\n",
    "        dim_data = clean_text(row.get(\"attribute_data:Dimensions:\", \"\"))\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Title\": title,\n",
    "            \"ProductName\": productname,\n",
    "            \"producturl\": producturl,\n",
    "            \"Brand\": brand_txt,\n",
    "            \"Type\": type_txt,\n",
    "            \"Category\": category_txt,\n",
    "            \"Description\": desc,\n",
    "            \"tax:product_tag\": tags_norm,\n",
    "            \"ImageURLs\": \",\".join(main_urls),\n",
    "            \"attribute:Dimensions:\": dim_text,\n",
    "            \"attribute_data:Dimensions:\": dim_data,\n",
    "            \"DimensionImages\": \",\".join(mapped_dim_urls)\n",
    "        })\n",
    "\n",
    "    out_cols = [\n",
    "        \"Title\", \"ProductName\", \"producturl\", \"Brand\", \"Type\", \"Category\",\n",
    "        \"Description\", \"tax:product_tag\", \"ImageURLs\",\n",
    "        \"attribute:Dimensions:\", \"attribute_data:Dimensions:\", \"DimensionImages\"\n",
    "    ]\n",
    "    pd.DataFrame(out_rows, columns=out_cols).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Mapped CSV saved as '{OUTPUT_CSV}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_updated_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602acde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a0648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
